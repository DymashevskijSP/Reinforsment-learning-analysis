# -*- coding: utf-8 -*-
"""DDPG_solved.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N7Bh_wiJKizCAsUD6lbXSs4y4eFr-_3X

*Установка необходимых библиотек: ~30 сек*
"""

!pip install box2d-py > /dev/null 2>&1
!pip install gym[Box_2D] > /dev/null 2>&1
!pip install tqdm > /dev/null 2>&1
!pip install matplotlib > /dev/null 2>&1
!pip install torch > /dev/null 2>&1
!pip install numpy > /dev/null 2>&1
!pip install gym pyvirtualdisplay > /dev/null 2>&1
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1
!pip install pyglet==1.2.4 > /dev/null 2>&1

"""*Импортим все что нужно:*"""

import gym
import os
import matplotlib.pyplot as plt
import copy
from tqdm import tqdm_notebook as tqdm
import random
import numpy as np
from collections import deque
import torch
import contextlib
from torch import nn
from torch import optim
import torch.nn.functional as F

"""Определяем константы. Мы напишем некоторый общий интерфейс для запуска любой среды из `gym`, однако для каждого энвайромента подойдут свои гиперпараметры, их надо настраивать отдельно для каждой среды.

- **Easy level**: `MountainCarContinuous-v0` \\
Машинка заезжает на гору. Дают 100 если заехать на флажок и отнимают за большее по величине действие. Необходимо стабильно набирать 90 реварда. [Подробнее тут](https://github.com/openai/gym/wiki/MountainCarContinuous-v0) \\
![alt text](https://user-images.githubusercontent.com/8510097/31701297-3ebf291c-b384-11e7-8289-24f1d392fb48.PNG)
- **Medium level**: `LunarLanderContinuous-v2` \\
Задача - посадить корабль. От 100 до 140 если как можно плавнее приблизиться к флажкам. Дают (или отнимают) 100 если не разбились. Еще +10 за каждую призимлившуюся ногу.  Необходимо стабильно набирать 200 реварда. [Подробнее тут](https://github.com/openai/gym/wiki/Leaderboard#LunarLanderContinuous-v2) \\
![alt text](https://i.ytimg.com/vi/vb15JFOEtLg/hqdefault.jpg)
- **Hard level**: `BipedalWalker-v2` \\
Необходимо научить робота ходить. Награда дается за прохождение вперед, можно получить более 300. На действие раcходуется "энергия", чем меньше потратите - тем меньше реварда у вас отнимут. Необходимо стабильно набирать 300 реварда (или хотя бы около того). [Подробнее тут](https://github.com/openai/gym/wiki/BipedalWalker-v2) \\
![alt text](https://i.ytimg.com/vi/QW6fWP5FDoU/hqdefault.jpg)
"""

# "MountainCarContinuous-v0" or "LunarLanderContinuous-v2" or "BipedalWalker-v2"
ENV_ID = "BipedalWalker-v2"

if ENV_ID == "MountainCarContinuous-v0":
    GAMMA = 0.99
    TAU = 0.05
    BATCH_SIZE = 256
    ACTOR_LR = 1e-5
    CRITIC_LR = 1e-5
    MAX_EPISODES = 35
    MAX_TIMESTAMPS = 500
    SIGMA = 0.6
    EPS_MIN = 0.1  # from [0...1]
    ALGORITHM = 'DDPG'  # 'DDPG' or 'TD3'
    
elif ENV_ID == "LunarLanderContinuous-v2":
    GAMMA = 0.99
    TAU = 0.005
    BATCH_SIZE = 256
    ACTOR_LR = 1e-5
    CRITIC_LR = 1e-4
    MAX_EPISODES = 150
    MAX_TIMESTAMPS = 500
    SIGMA = 0.4
    EPS_MIN = 0.1  # from [0...1]
    ALGORITHM = 'DDPG'  # 'DDPG' or 'TD3'
    
elif ENV_ID == "BipedalWalker-v2":
    GAMMA = 0.99
    TAU = 0.05
    BATCH_SIZE = 256
    ACTOR_LR = 1e-3
    CRITIC_LR = 5e-4
    MAX_EPISODES = 300
    MAX_TIMESTAMPS = 500
    SIGMA = 0.6
    EPS_MIN = 0.2  # from [0...1]
    ALGORITHM = 'TD3'  # 'DDPG' or 'TD3'

CAPACITY = 50000
EXPLOIT_INTERVAL = MAX_EPISODES // 50 if MAX_EPISODES >= 50 else 1
EXPLOIT_EPISODES = 3
RERUN_EPISODES = 30
SEED = 1234
DEVICE = 'gpu'  # 'gpu' or 'cpu'
TD3_POLICY_DELAY = 2

if not torch.cuda.is_available() and DEVICE == 'gpu':
    print('No gpu detected, using cpu...')
    DEVICE = torch.device('cpu')
else:
    DEVICE = torch.device("cuda:0" if DEVICE == 'gpu' else "cpu")
if not os.path.exists('./weights'):
    os.mkdir('weights')
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

"""Парочка вспомогательных функций для перевода `torch` тензоров в `numpy` массивы и обратно. `numpy` это библиотека линейной аглебры, позволяющая быстро и лаконично записать векторизованные операции с помощью функций над многомерными массивами, однако она не поддерживает gpu, а значит не может быть эффективно ускорена на видеокартах. Поэтому для ускорения прохождения данных через глубокие нейронные сети и их обучения ипользуется `torch` тензоры.

`tensor.detach()` создает тензор, который делит память с `tensor` и не требует просчета градиентов (поле `requires_grad=False`). Используем этот метод если хотим убрать тензор из вычислительного графа, и не планируем далее использовать `.backward()`. Подробнее [PyTorch .detach() method](http://www.bnikolic.co.uk/blog/pytorch-detach.html)
"""

def to_torch(x, dtype=torch.float):
    return torch.tensor(x, dtype=dtype, device=DEVICE)

def to_numpy(x):
    return x.detach().cpu().numpy()

"""Различные виды шума $N$, которые мы будем добавлять к действию $a$ для exploration'а среды на начальном этапе: $a \to a + N$. Каким свойствам должна удолетворять случайная величина $N$?

- Нулевое матожидание $E[N] = 0$. Это означает что в среднем наш шум не смещает действие агента. Это нужно для равномерного exploration'а среды.
- Уменьшающаяся дисперсия $\sigma^2_t > \sigma^2_{t+1}$. Дисперсия по-сути связана со средней величиной шума. Мы хотим, чтобы эта величина со временем уменьшалась (по мере обучения сетки), чтобы меньше полагаться на случайность для исследования среды и больше на выбор агента. Для этого мы вводим поле `eps` на которое будем умножать шум и которое мы будем линейно уменьшать каждый эпизод от `eps_max` до `eps_min`.

Нормальная гаусовая величина с нулевым средним или процесс орнштейна-уленбека.

Каждая среда устанавливает свои пределы для действия агента. Чтобы не залезать за них после добавления шума, клипаем действие с помощью `np.clip`.
"""

class OUNoise:
    def __init__(self, action_space):
        self.theta = 0.15
        self.sigma = SIGMA
        self.action_dim = action_space.shape[0]
        self.low = action_space.low[0]
        self.high = action_space.high[0]
        self.eps_max = 1.0
        self.eps_min = EPS_MIN
        self.eps = self.eps_max
        self.eps_decay = (self.eps_max - self.eps_min) / MAX_EPISODES
        self.state = np.zeros(self.action_dim)

    def eps_step(self):
        self.eps -= self.eps_decay

    def __call__(self, action, clip=False):
        self.state += - self.theta * self.state + self.sigma * np.random.randn(self.action_dim)
        noise = self.eps * self.state
        noise = np.clip(noise, -2 * SIGMA, 2 * SIGMA) if clip else noise
        if isinstance(action, torch.Tensor):
            return torch.clamp(action + to_torch(noise), self.low, self.high)
        else:
            return np.clip(action + noise, self.low, self.high)

"""Классы для Actor'а и Critic'а."""

class DDPGActor(nn.Module):
    def __init__(self, obs_size, act_size):
        super(DDPGActor, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(obs_size, 400),
            nn.LayerNorm(400),
            nn.ReLU(),
            nn.Linear(400, 300),
            nn.LayerNorm(300),
            nn.ReLU(),
            nn.Linear(300, act_size),
            nn.Tanh(),
        )
    
    def forward(self, x):
        return self.net(x)
    
    def make_action(self, state):
        state = to_torch(state)
        action = self.forward(state)
        return to_numpy(action)


class DDPGCritic(nn.Module):
    def __init__(self, obs_size, act_size):
        super(DDPGCritic, self).__init__()

        self.obs_net = nn.Sequential(
            nn.Linear(obs_size, 400),
            nn.LayerNorm(400),
            nn.ReLU(),
        )

        self.out_net = nn.Sequential(
            nn.Linear(400 + act_size, 300),
            nn.LayerNorm(300),
            nn.ReLU(),
            nn.Linear(300, 1),
        )

    def forward(self, state, action):
        obs = self.obs_net(state)
        return self.out_net(torch.cat([obs, action], dim=1))

"""Функция для еденичного апдейта сеток актора и критика с помощью запомненым ранее кортежей переходов из стейта в стейт `replay_buffer`.

Сначала сэмплим из него `BATCH_SIZE` экземпляров переходов. Затем нам надо взять одинаковые сущности (состояния, действия, награды...) из этих кортежей и обьеденить в один тензор для быстрого и корректного прогона через сетки.

Далее считаем ожидаемые значений Q-функции `expected_qvalue` и те которые имеем на самом деле `qvalue`. Обратите внимание что после прогона через таргет сетки мы скастовали `.detach()` потому что не собирались просчитывать градиенты через них. Таргет сетки мы обновляем мягко, через конктанту `TAU` в самом конце.

Затем считаем `loss` для критика, просчитываем градиенты и обновляем веса. Тоже самое и для Актора.
"""

def ddpg_update(actor, critic, replay_buffer):
    
    # sample from memory
    state, action, reward, next_state, done = \
        [to_torch(e) for e in zip(*random.sample(replay_buffer, BATCH_SIZE))]

    # get qvalue + target qvalue
    expected_qvalue = critic.target(next_state, actor.target(next_state))  
    expected_qvalue = reward.unsqueeze(1) + (1.0 - done.unsqueeze(1)) * GAMMA * expected_qvalue
    expected_qvalue = expected_qvalue.detach()
    qvalue = critic(state, action)

    # update critic
    critic_loss = F.mse_loss(qvalue, expected_qvalue)
    critic.optimizer.zero_grad()
    critic_loss.backward()
    critic.optimizer.step()
    
    # update actor
    actor_loss = - critic(state, actor(state)).mean()
    actor.optimizer.zero_grad()
    actor_loss.backward()
    actor.optimizer.step()

    # update target networks
    for target_param, param in zip(critic.target.parameters(), critic.parameters()):
        target_param.data.copy_(target_param.data * (1.0 - TAU) + param.data * TAU)
    for target_param, param in zip(actor.target.parameters(), actor.parameters()):
        target_param.data.copy_(target_param.data * (1.0 - TAU) + param.data * TAU)
    
    return actor, critic, replay_buffer


def td3_update(actor, critic, noise, step, replay_buffer):
    
    # sample from memory
    state, action, reward, next_state, done = \
        [to_torch(e) for e in zip(*random.sample(replay_buffer, BATCH_SIZE))]
    
    # unpack 2 critics
    critic1, critic2 = critic
        
    # get qvalue + target qvalue   
    target_actions = noise(actor.target(next_state), clip=True)
    expected_qvalue = torch.min(
        critic1.target(next_state, target_actions),
        critic2.target(next_state, target_actions),
    )
    expected_qvalue = reward.unsqueeze(1) + (1.0 - done.unsqueeze(1)) * GAMMA * expected_qvalue
    expected_qvalue = expected_qvalue.detach()
    qvalue1 = critic1(state, action)
    qvalue2 = critic2(state, action)

    # update critic1
    
    critic_loss1 = F.mse_loss(qvalue1, expected_qvalue)
    critic1.optimizer.zero_grad()
    critic_loss1.backward()
    critic1.optimizer.step()
    
    # update critic2
    critic_loss2 = F.mse_loss(qvalue2, expected_qvalue)
    critic2.optimizer.zero_grad()
    critic_loss2.backward()
    critic2.optimizer.step()

    if step % TD3_POLICY_DELAY == 0:
    
        # update actor
        actor_loss = - critic1(state, actor(state)).mean()
        actor.optimizer.zero_grad()
        actor_loss.backward()
        actor.optimizer.step()

        # update target networks
        for target_param, param in zip(critic1.target.parameters(), critic1.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - TAU) + param.data * TAU)
        for target_param, param in zip(critic2.target.parameters(), critic2.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - TAU) + param.data * TAU)
        for target_param, param in zip(actor.target.parameters(), actor.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - TAU) + param.data * TAU)
    
    # pack 2 critics
    critic = (critic1, critic2)

    return actor, critic, noise, replay_buffer

"""Функция для тестирования таргет актора. Опять же не забываем про клип действия."""

def exploit_episode(env, actor_tgt, render=False):
    state = env.reset() 
    done, ep_reward = False, 0
    while not done:
        if render: env.render()
        action = actor_tgt.make_action(state)
        action = np.clip(action, env.action_space.low, env.action_space.high)
        state, reward, done, _ = env.step(action)
        ep_reward += reward
    return ep_reward


def exploit(env, actor_tgt, num, render=False):
    return [exploit_episode(env, actor_tgt, render) for _ in range(num)]

"""Играем и обучаем один эпизод"""

def play_episode(noise, actor, critic, replay_buffer):
    state = env.reset()
    noise.eps_step()
    ep_reward = 0

    for step in range(MAX_TIMESTAMPS):
    
        # make step
        action = noise(actor.make_action(state))
        next_state, reward, done, _ = env.step(action)

        # append in replay buffer, update state
        replay_buffer.append((state, action, reward, next_state, done))
        state = next_state
        ep_reward += reward

        # gradient decent nets update
        if len(replay_buffer) > BATCH_SIZE:
            if ALGORITHM == 'DDPG':
                actor, critic, replay_buffer = ddpg_update(actor, critic, replay_buffer)
            elif ALGORITHM == 'TD3':
                actor, critic, noise, replay_buffer = td3_update(actor, critic, noise, step, replay_buffer)
        
        if done:
            break
    
    return ep_reward, noise, actor, critic, replay_buffer

"""Вспомогательные функции для обучения. Тут лучше ничего не менять.
- Сохранение и загрузка весов для таргет версии актора, которая и используется при тесте модели.
- Проигрывание видео (запуститься прямо в ячейке) лучшего эпизода по названию файла с сохранеными весами.
- Графики реварда на эпизодах обучения и теста.
"""

# Commented out IPython magic to ensure Python compatibility.
def save(id_episode, actor_tgt, reward):
    weights_name = f"{ENV_ID}_ep={id_episode}_r={reward:.2f}"
    torch.save(actor_tgt.state_dict(), f"weights/{weights_name}")
    return weights_name


def load(actor_tgt, weights_name):
    state_dict = torch.load(f"weights/{weights_name}", map_location=lambda storage, location: storage)
    actor_tgt.load_state_dict(state_dict)
    return actor_tgt

def show_video(weights_name, env_name=None):
    
    from gym.wrappers import Monitor
    import tensorflow as tf
    import numpy as np
    import random
    import matplotlib
    import matplotlib.pyplot as plt
#     %matplotlib inline
    import math
    import glob
    import io
    import base64
    from IPython.display import HTML
    from IPython import display as ipythondisplay
    from pyvirtualdisplay import Display
    display = Display(visible=0, size=(1400, 900))
    display.start()
    
    if env_name is None:
        env_name = weights_name.split('_')[0]

    def aux(env, i=0):
        video = io.open(f'./video/openaigym.video.%s.video00000{i}.mp4' % env.file_infix, 'r+b').read()
        encoded = base64.b64encode(video)
        ipythondisplay.display(HTML(data='''<video alt="test" autoplay 
                    loop controls style="height: 400px;">
                    <source src="data:video/mp4;base64,{0}" type="video/mp4" />
                 </video>'''.format(encoded.decode('ascii'))))

    with contextlib.redirect_stdout(None):
        env = Monitor(gym.make(env_name), './video', force=True, video_callable=lambda episode_id: True)
    actor_tgt = DDPGActor(env.observation_space.shape[0], env.action_space.shape[0]).to(DEVICE)
    if weights_name is not None:
        actor_tgt = load(actor_tgt, weights_name)
    
    exploit_rewards = exploit(env, actor_tgt, num=EXPLOIT_EPISODES, render=True)
    print(f"Playing video with reward: {np.max(exploit_rewards):.3f}")
    aux(env, i=np.argmax(exploit_rewards))

def rerun_best(weights_name):
    
    env_name = weights_name.split('_')[0]  
    with contextlib.redirect_stdout(None):
        env = gym.make(env_name)
    actor_tgt = DDPGActor(env.observation_space.shape[0], env.action_space.shape[0]).to(DEVICE)
    actor_tgt = load(actor_tgt, weights_name)
    exploit_rewards = exploit(env, actor_tgt, num=RERUN_EPISODES)
    print(f"Rerunning best target model {RERUN_EPISODES} times: mean_r={np.mean(exploit_rewards):.3f}")
    
def show_plots(train_rewards, exploit_rewards):
    train_times = [x for x in range(1, MAX_EPISODES + 1)]
    exploit_times = [x for x in range(1, MAX_EPISODES + 1) if x % EXPLOIT_INTERVAL == 0]
    plt.figure(figsize=(10, 8))
    plt.plot(train_times, train_rewards, label='Train Rewards')
    plt.plot(exploit_times, exploit_rewards, label='Exploit Rewards')
    plt.legend(loc=4, prop={'size': 18})
    plt.show()

"""Определяем все необходимые обьекты и инструменты для обучения.

Загнали создание среды в `contextlib.redirect_stdout(None)` чтобы оно не выводило дурацкое сообщение о каком-то предупреждении. (связано с криво написаным `gym`). Сразу же смотрим на то сколько чисел в векторе состояния и действия, а также каковы его пределы.

Далее определяем тип используемого шума.

Для определения актора, сналача создаем инстанс класса `DDPGActor` для локального измения с помощью градиентного спуска. Затем копируем таргет версию той же сетки, которую будем обновлять мягко. Главная задача обучить именно таргет сетку. Также создаем `optimizer`, `Adam` вполне подойдет для любых задач. Теперь чтобы не протаскивать эти 3 элемента через все функции, которые по-сути относятся к одной и той же сущности - актору, мы заводим переменную `actor`, которая будет в точности локальной версией сетки и заводим у нее еще 2 поля (изящность питона позволяет сделать это без дополнительных конструкций). Делается это для компактонсти записи и улучшения читаемости кода.

Тоже самое и с критиком.

Заводим буффер для хранения прошлых транзитов. Это просто дека, ограниченная по вместимости, чтобы отбрасывать совсем древние элементы.
"""

def get_tools():
    with contextlib.redirect_stdout(None):
        env = gym.make(ENV_ID)
        env.seed(SEED)
    print(f"{ENV_ID}\n"
          f"observation space: {env.observation_space}\n"
          f"action space: {env.action_space}\n"
          f"action range: ({env.action_space.low[0]}, {env.action_space.high[0]})\n")

    # define noise
    noise = OUNoise(env.action_space)

    # define actor   
    actor_local = DDPGActor(env.observation_space.shape[0], env.action_space.shape[0]).to(DEVICE)
    actor_target = copy.deepcopy(actor_local)
    actor_optimizer = optim.Adam(actor_local.parameters(), lr=ACTOR_LR)
    actor = actor_local
    actor.target = actor_target
    actor.optimizer = actor_optimizer
    
    if ALGORITHM == 'DDPG':        
        # define critic
        critic_local = DDPGCritic(env.observation_space.shape[0], env.action_space.shape[0]).to(DEVICE)
        critic_target = copy.deepcopy(critic_local)
        critic_optimizer = optim.Adam(critic_local.parameters(), lr=CRITIC_LR)
        critic = critic_local
        critic.target = critic_target
        critic.optimizer = critic_optimizer

    if ALGORITHM == 'TD3':
         # define critic1
        critic_local1 = DDPGCritic(env.observation_space.shape[0], env.action_space.shape[0]).to(DEVICE)
        critic_target1 = copy.deepcopy(critic_local1)
        critic_optimizer1 = optim.Adam(critic_local1.parameters(), lr=CRITIC_LR)
        critic1 = critic_local1
        critic1.target = critic_target1
        critic1.optimizer = critic_optimizer1
        
        # define critic2
        critic_local2 = DDPGCritic(env.observation_space.shape[0], env.action_space.shape[0]).to(DEVICE)
        critic_target2 = copy.deepcopy(critic_local2)
        critic_optimizer2 = optim.Adam(critic_local2.parameters(), lr=CRITIC_LR)
        critic2 = critic_local2
        critic2.target = critic_target2
        critic2.optimizer = critic_optimizer2
        critic = (critic1, critic2)

    # define replay buffer
    replay_buffer = deque(maxlen=CAPACITY)
    
    return env, noise, actor, critic, replay_buffer

"""траин драфт

Модель тестируется каждый `EXPLOIT_INTERVAL` эпизод, для этого играется дополнительная игра, без обучения модели.
"""

def train_draft(env, noise, actor, critic, replay_buffer):
    train_rewards = []
    exploit_rewards = []
    best_episode_weights = None
    best_exploit_reward = -np.inf

    pbar = tqdm(range(1, MAX_EPISODES + 1), total=MAX_EPISODES)
    for id_episode in pbar:

        # play + train episode
        ep_reward, noise, actor, critic, replay_buffer = \
            play_episode(noise, actor, critic, replay_buffer)

        # record episode reward
        train_rewards.append(ep_reward)
        pbar.set_description(f"R: {train_rewards[-1]:.3f}")

        # evaluate by exploitation
        if id_episode % EXPLOIT_INTERVAL == 0:
            exploit_rewards.append(
                np.mean(exploit(env, actor.target, num=EXPLOIT_EPISODES))
            )
            print(f"episode: {id_episode} | exploit reward: {exploit_rewards[-1]:.3f}")
            if exploit_rewards[-1] > best_exploit_reward:
                best_exploit_reward = exploit_rewards[-1]
                best_episode_weights = save(id_episode, actor.target, exploit_rewards[-1])

    print(f"Weights for best episode: {best_episode_weights}")
    return train_rewards, exploit_rewards, best_episode_weights













"""## Training

Если индикатор обучения из tqdm не отобразился - перезапустите ячейку
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# env, noise, actor, critic, replay_buffer = get_tools()
# train_rewards, exploit_rewards, best_weights = train_draft(env, noise, actor, critic, replay_buffer)
# rerun_best(best_weights)

"""## Plots"""

show_plots(train_rewards, exploit_rewards)

"""## Video"""

show_video(best_weights)

